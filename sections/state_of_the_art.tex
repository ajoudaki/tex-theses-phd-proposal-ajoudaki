
On of the popular distance metrics used in computer science and computational biology is Levenstein's distance, also referred to as \emph{the edit distance}. Despite decades of research for faster exact algorithms, the quadratic time of the dynamic programming solution\cite{Vintsyuk1968SpeechProgramming}, has been only reduced to $\Ocal(n^2/\log^2 n)$, which essentially remains quadratic in the string length $n$ \cite{Masek1980ADistances}. This has spurred research on approximation algorithms, starting with a linear-time $\sqrt{n}$-approximation\footnote{An approximation factor $C$, implies computing an estimate metric $\D(x,y)$, that satisfies ${\ED(x,y)\le \D(x,y)\le C \cdot \ED(x,y)}$} by Meyers and Schmidt\cite{Landau1998IncrementalComparison}. Further research on quasi-linear\footnote{It means linear up to a polylogarithmic factor in input size $\Ocal(n\cdot \mathrm{Polylog}(n))$} algorithms has reduced this factor to $n^{3/7}$ by Bar-Yossef et al.~\cite{Bar-Yossef2004ApproximatingEfficiently}, to $n^{1/3+o(1)}$ by Batu et al.~\cite{batu2006oblivious}, and then vastly improved to $2^{\tilde\Ocal(\sqrt{n})}$ and ${(\log n)}^{\Ocal(1/\varepsilon)}$ by Andoni et al.~\cite{Andoni2012ApproximatingTime,Andoni2010PolylogarithmicComplexity}.  Finally, the only known strongly sub-quadratic result with an absolute constant approximation factor runs in $n^{2-2/7}$~\cite{Time2018ApproximatingTime}. 

Given input metric space $(X,\D_X)$, and sketch space $(Y,\D_Y)$, sketching is a randomized mapping $\Phi:X\to Y$ where  $\D_Y(\Phi(x),\Phi(y))$ approximates the original distance $\D_X(x,y)$. If $\D_Y$ is a well-understood metric such as Euclidean distance $\|\varphi(x)-\varphi(y)\|_2$, sketching will automatically lead to more efficient neighbor search and clustering algorithms~\cite{Indyk1998ApproximateNeighbors}. The currently best embedding result for the edit distance remains the $\ell^1$-embedding by Ostrovsky and Rabani which gives a $2^{\tilde\Ocal(\sqrt{n})}$-approximation, but its practical relevance remains unknown.  


The edit distance and its variants were first introduced to bioinformatics by Smith and Waterman~\cite{Smith1981IdentificationSubsequences}, since the edit operations can model \emph{single nucleotide polymorphism (SNP)}, \emph{indels (insertion or deletion)}, or sequencing errors in the DNA. However, due to the high cost of exact computation, several state-of-the-art tools rely on $k$-mer frequency~\cite{mapleson2016kat}, spaced $k$-mer frequency~\cite{morgenstern2019sequence}, and maximal matching substrings~\cite{thankachan2017greedy}, in order to estimate evolutionary distances. These distances and the position of substring matches can be used for phylogeny construction~\cite{thankachan2017greedy}, clustering multiple sequences~\cite{ondov2016mash}. A related problem is dealing with sequencing noise, for read to reference mapping~\cite{chakraborty2019s}, and \emph{de novo assembly}~\cite{li2016minimap}. 


Evolutionary changes in the genome beyond SNPs and indels, such as inversion, duplication, and translocation, also referred to as \emph{structural variations}, cannot be modeled correctly as edit operations. Therefore, tools such as SyRI~\cite{goel2019syri}, find rearranged subsequences in assembled sequences. Moreover, these structural differences are key to visualize assembled genome on different scales~\cite{Yokoyama2019MoMI}. 

Sketching techniques have been successfully employed to lower the computational burden of many algorithms, such as fast \emph{Johnson-Lindenstrauss (JL)} dimensionality reduction~\cite{AILON2007THENEIGHBORS}, low-rank matrix factorization~\cite{tropp2017practical,yurtsever2017sketchy}, fitting kernel machines~\cite{rahimi2008random}, graph sparsification~\cite{spielman2011spectral}, and solving linear programs~\cite{cohen2019solving}. Finally, efficient sketching of low rank tensors, can be exploited to fit polynomial kernels more efficiently~\cite{pham2013fast}. 


\subsection{Motivation}

% The increase in capacity of high-throughput next-generation sequencing technologies, has created a demand for efficient and scalable algorithms. 
The size of datasets produced by  high-throughput next-generation sequencing technologies, typically exceeds the available memory, which renders algorithms that require random access to the entire dataset inapplicable.
Therefore, in one or a few passes over the sequence, the most relevant information must be extracted and stored. 
This summary information must suffice to approximately answer queries relevant to the application.  
Sketching and streaming algorithms follow a similar paradigm, and often lead to practical and easy to implement algorithms. This motivates a similar approach to design efficient algorithms for important problems in bioinformatics. 
