
The main ideas sketching algorithms rely on, has been developed in different domains, such as the study of randomized algorithms and probabilistic methods, similarity preserving embeddings, compressive sensing, and sensitive hashing. For instance in compressive sensing, the discovery that sparse signal recovery requires fewer samples than predicted by the Nyquist-Shannon sampling theorem has allowed faster MRI acquisition by requiring fewer samples. In a nutshell, given the prior knowledge about sparsity and with the right sensor design, the recovery can be relaxed and solved via $\ell^1$-minimization methods, such as iterative thresholding. Other notable examples are min-hash, initially developed for detection of duplicate documents, and Bloom filters, developed for memory-efficient set membership queries when some false positives are acceptable. Despite the original use cases, both methods have found application in bioinformatics for classifying sequences and analysis when exact similarity estimation is not feasible. 


The techniques introduced above, have been utilized to lower the complexity of many important algorithms and optimization procedures, such as low-dimensional random features, convex optimization, and symmetric tensor sketching.
One of the main advantages of these probabilistic algorithms is that the accuracy-complexity trade-off can be computed, allowing us to design algorithms that are more predictable, interpretable, robust, and have fewer unknown quantities than more sophisticated models such as random forests and deep neural networks. 
Therefore, revisiting these mathematically rigorous but computationally inexpensive models can potentially lead to faster and more robust algorithms for bioinformatics application often working with terabytes of data.



Due to the sheer scale and heterogeneous sources of variation in a genome, the exact alignment, or similarity estimation of sequencing reads is computationally intractable. This motivates us to explore methods for fast sequence analysis that are scalable for large sequence datasets. Most bioinformatics tools rely on prior knowledge about the data and use heuristics tailored to the specific requirements of each application. 
However, relying on heuristics makes it harder to interpret the results, since it is not possible to know how sub-optimal the results are. 
In multiple sequence alignment, there is a complete lack of a gold-standard method with a run-time not growing exponentially with respect to the number of sequences. On the other hand, the assumptions for optimality of sketching-based algorithms are often explicit, and the sub-optimality ratio can be traded for time and memory complexity. 



The connecting theme of research goals outlined in this text is the sketching distance of metrics into $\ell^2$, $\ell^1$, or Hamming distance metrics. These sketches will automatically lead to efficient nearest neighbor algorithms~\cite{datar2004locality}. Moreover, operating on vectors or bit-strings, can be optimized at the CPU or GPU level. The research goals and work packages follow a natural progression, since the main ideas in some sections are developed on top of the previous ones. 


The first work package concerns sketching the similarity of strings. The edit operations, as defined by the edit distance, can be a good proxy to evolutionary distances, caused by mutations. As an application, these sketches can be used to cluster short sequences belonging to multiple species, and to estimate distances between these genomes. The second step is to use the sketches and nearest neighbor queries, for read to reference alignment, using a seed and extend strategy. The sketches can be used to find the best seed, even if the seed contains a few point mutations. While the edit operations closely model genomic changes on the single nucleotides, the structural changes, such as translocation, inversion, or duplication, operate on a different scale. Therefore, in the third step we plan to build a syntenic sketch on top of similarity sketches, that captures such changes to the genome. The sketches can be used to find syntenic blocks and regions, as well as find syntenic alignments using nearest neighbor queries. Finally, one may generalize the two-level sketching for syntenic regions, to a multi-level recursive sketch. These sketches, combined with de-Brujin graph representation of sequences, may shed new light on genomic structural differences, and help to visualize them. 

