
The most popular distance metric used in computer science and computational biology is Levenstein's distance, also referred to as \emph{the edit distance}, denoted by $\ED$ in this text. \cite{Levenshtein1966BinaryReversals}. Despite decades of research for faster exact algorithms, the quadratic time of the dynamic programming solution\cite{Vintsyuk1968SpeechProgramming}, has been only reduced to $\Ocal(n^2/\log^2 n)$, which essentially remains quadratic in the string length $n$ \cite{Masek1980ADistances}. This has spurred research on approximation algorithms, starting with a linear-time $\sqrt{n}$-approximation\footnote{An approximation factor $C$, implies computing an estimate metric $\D(x,y)$, that satisfies ${\ED(x,y)\le \D(x,y)\le C \cdot \ED(x,y)}$} by Meyers and Schmidt\cite{Landau1998IncrementalComparison}. Further research on quasi-linear\footnote{It means linear up to a polylogarithmic factor in input size $\Ocal(n\cdot \mathrm{Polylog}(n))$} algorithms has reduced this factor to $n^{3/7}$ by Bar-Yossef et al.~\cite{Bar-Yossef2004ApproximatingEfficiently}, to $n^{1/3+o(1)}$ by Batu et al.~\cite{batu2006oblivious}, and then vastly improved to $2^{\tilde\Ocal(\sqrt{n})}$ and ${(\log n)}^{\Ocal(1/\varepsilon)}$ by Andoni et al.~\cite{Andoni2012ApproximatingTime,Andoni2010PolylogarithmicComplexity}.  Finally, the only known strongly sub-quadratic result with an absolute constant approximation factor runs in $n^{2-2/7}$~\cite{Time2018ApproximatingTime}. 

Sketching the edit distance is a related and equally important problem. The goal is to  function that maps strings $x,y\in \abc^n$ to a binary string $\varphi: \abc^n\to \{0,1\}^m$, such that some distance between sketches, namely their hamming distance $\HD(\varphi(x),\varphi(y))$, approximates the edit distance $\ED(x,y)$, analogous to the approximation algorithms. For example an embedding into $\ell^1$ or Hamming cube, automatically leads to more efficient neighbor search algorithms, due to classical results by Indyk et al.~\cite{Indyk1998ApproximateNeighbors}. The best embedding result for the edit distance remains the $\ell^1$-embedding by Ostrovsky and Rabani with gives a $2^{\tilde\Ocal(\sqrt{n})}$-approximation . Eventhough this embedding has inspired some of the state-of-the-art results on approximating the edit distance, it is not known if the embedding itself can be computed efficiently. 


The edit distance and its variants, were first introduced to bioinformatics by Smith and Waterman~\cite{Smith1981IdentificationSubsequences}, since the edit operations can model \emph{single nucleotide polymorphism (SNP)}, \emph{indels (insertion or deletion)}, or sequencing errors in the DNA. However, due to the high cost of exact computation, several state-of-the-art tools rely on $k$-mers frequency~\cite{mapleson2016kat}, spaced $k$-mer frequency~\cite{morgenstern2019sequence}, and maximal matching substrings~\cite{thankachan2017greedy}, in order to estimate evolutionary distances. These distances and the position of substring matches can be used for phylogeny construction~\cite{thankachan2017greedy}, clustering multiple sequences~\cite{ondov2016mash}, read to reference mapping~\cite{chakraborty2019s}, and \emph{de novo assembly}~\cite{li2016minimap}. 


Evolutionary changes in the genome beyond SNPs and indels, such as inversion, duplication, and translocation, cannot be modeled correctly as edit operations, also referred to as \emph{structural variations}. Therefore, tools such as SyRI~\cite{goel2019syri}, find rearranged subsequences in assembled sequences, which might reside in a \emph{syntenic region}. Moreover, these structural differences are key to visualize assembled genome on different scales~\cite{Yokoyama2019MoMI}. 


Finally, sketching techniques have been successfully employed to lower the computational burden of many algorithms, such as fast \emph{Johnson-Lindenstrauss (JL)} dimensionality reduction~\cite{AILON2007THENEIGHBORS}, low-rank matrix factorization~\cite{tropp2017practical,yurtsever2017sketchy}, fitting kernel machines~\cite{rahimi2008random}, graph sparsification~\cite{spielman2011spectral}, and solving linear programs~\cite{cohen2019solving}, and efficient sketching of low rank tensors, can be exploited to fit polynomial kernels more efficiently~\cite{pham2013fast}. 


\subsection{Motivation}

% The increase in capacity of high-throughput next-generation sequencing technologies, has created a demand for efficient and scalable algorithms. 
The size of datasets produced by  high-throughput next-generation sequencing technologies, typically exceeds the available memory, which renders algorithms that require random access to the entire dataset inapplicable.
Therefore, in one or a few passes over the sequence, the most relevant information must be extracted and stored. 
This summary information must suffice to approximately answer queries relevant to the application.  
Sketching and streaming algorithms follow a similar paradigm, and often lead to practical and easy to implement algorithms. This motivates us take to a similar approach to design efficient algorithms for important problems in bioinformatics. 
